{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5f1f864",
   "metadata": {},
   "source": [
    "# An intuition to SVMs\n",
    "\n",
    "The linear classification model expects the data to be linearly separable. \n",
    "When this assumption does not hold, the model is not expressive enough to properly fit the data.\n",
    "Therefore, we need to apply the same tricks as in regression: feature\n",
    "augmentation (potentially using expert-knowledge) or using a\n",
    "kernel-based method.\n",
    "\n",
    "We will provide examples where we will use a kernel support vector machine\n",
    "to perform classification on some toy-datasets where it is impossible to\n",
    "find a perfect linear separation.\n",
    "\n",
    "First, we redefine our plotting utility to show the decision boundary of a\n",
    "classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988470f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_decision_function(fitted_classifier, range_features, ax=None):\n",
    "    \"\"\"Plot the boundary of the decision function of a classifier.\"\"\"\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    feature_names = list(range_features.keys())\n",
    "    # create a grid to evaluate all possible samples\n",
    "    plot_step = 0.02\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.arange(*range_features[feature_names[0]], plot_step),\n",
    "        np.arange(*range_features[feature_names[1]], plot_step),\n",
    "    )\n",
    "\n",
    "    # compute the associated prediction\n",
    "    Z = fitted_classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = LabelEncoder().fit_transform(Z)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # make the plot of the boundary and the data samples\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    ax.contourf(xx, yy, Z, alpha=0.4, cmap=\"RdBu\")\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449a413e",
   "metadata": {},
   "source": [
    "We will generate a first dataset where the data are represented as two\n",
    "interlaced half circle. This dataset is generated using the function\n",
    "[`sklearn.datasets.make_moons`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6f82aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "feature_names = [\"Feature #0\", \"Features #1\"]\n",
    "target_name = \"class\"\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.13, random_state=42)\n",
    "\n",
    "# We store both the data and target in a dataframe to ease plotting\n",
    "moons = pd.DataFrame(np.concatenate([X, y[:, np.newaxis]], axis=1),\n",
    "                     columns=feature_names + [target_name])\n",
    "data_moons, target_moons = moons[feature_names], moons[target_name]\n",
    "\n",
    "range_features_moons = {\"Feature #0\": (-2, 2.5), \"Feature #1\": (-2, 2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2093ba",
   "metadata": {},
   "source": [
    "Since the dataset contains only two features, we can make a scatter plot to\n",
    "have a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca8f53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.scatterplot(data=moons, x=feature_names[0], y=feature_names[1],\n",
    "                hue=target_moons, palette=[\"tab:red\", \"tab:blue\"])\n",
    "_ = plt.title(\"Illustration of the moons dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d7232e",
   "metadata": {},
   "source": [
    "From the intuitions that we got by studying linear model, it should be\n",
    "obvious that a linear classifier will not be able to find a perfect decision\n",
    "function to separate the two classes.\n",
    "\n",
    "Let's try to see what is the decision boundary of such a linear classifier.\n",
    "We will create a predictive model by standardizing the dataset followed by\n",
    "a linear support vector machine classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a505faf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# TODO: implement a pipeline with standard scaler and SVC using linear kernel\n",
    "# TODO: one line of code is needed here\n",
    "linear_model.fit(data_moons, target_moons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce10bbef",
   "metadata": {},
   "source": [
    "Let's check the decision boundary of such a linear model on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b773d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(data=moons, x=feature_names[0], y=feature_names[1],\n",
    "                     hue=target_moons, palette=[\"tab:red\", \"tab:blue\"])\n",
    "plot_decision_function(linear_model, range_features_moons, ax=ax)\n",
    "_ = plt.title(\"Decision boundary of a linear model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de29a5cd",
   "metadata": {},
   "source": [
    "As expected, a linear decision boundary is not enough flexible to split the\n",
    "two classes.\n",
    "\n",
    "To push this example to the limit, we will create another dataset where\n",
    "samples of a class will be surrounded by samples from the other class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dcaf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "\n",
    "feature_names = [\"Feature #0\", \"Features #1\"]\n",
    "target_name = \"class\"\n",
    "\n",
    "X, y = make_gaussian_quantiles(\n",
    "    n_samples=100, n_features=2, n_classes=2, random_state=42)\n",
    "gauss = pd.DataFrame(np.concatenate([X, y[:, np.newaxis]], axis=1),\n",
    "                     columns=feature_names + [target_name])\n",
    "data_gauss, target_gauss = gauss[feature_names], gauss[target_name]\n",
    "\n",
    "range_features_gauss = {\"Feature #0\": (-4, 4), \"Feature #1\": (-4, 4)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa15dbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(data=gauss, x=feature_names[0], y=feature_names[1],\n",
    "                     hue=target_gauss, palette=[\"tab:red\", \"tab:blue\"])\n",
    "_ = plt.title(\"Illustration of the Gaussian quantiles dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a179cf4",
   "metadata": {},
   "source": [
    "Here, this is even more obvious that a linear decision function is not\n",
    "adapted. We can check what decision function, a linear support vector machine\n",
    "will find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a804f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.fit(data_gauss, target_gauss)\n",
    "ax = sns.scatterplot(data=gauss, x=feature_names[0], y=feature_names[1],\n",
    "                     hue=target_gauss, palette=[\"tab:red\", \"tab:blue\"])\n",
    "plot_decision_function(linear_model, range_features_gauss, ax=ax)\n",
    "_ = plt.title(\"Decision boundary of a linear model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27f27ad",
   "metadata": {},
   "source": [
    "As expected, a linear separation cannot be used to separate the classes\n",
    "properly: the model will under-fit as it will make errors even on\n",
    "the training set.\n",
    "\n",
    "Earlier we saw that we could use several\n",
    "tricks to make a linear model more flexible by augmenting features or\n",
    "using a kernel. Here, we will use the later solution by using a radial basis\n",
    "function (RBF) kernel together with a support vector machine classifier.\n",
    "\n",
    "We will repeat the two previous experiments and check the obtained decision\n",
    "function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dc58be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement a pipeline with standard scaler and SVC using rbf kernel\n",
    "# TODO: one line of code is required here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebb09a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_model.fit(data_moons, target_moons)\n",
    "ax = sns.scatterplot(data=moons, x=feature_names[0], y=feature_names[1],\n",
    "                     hue=target_moons, palette=[\"tab:red\", \"tab:blue\"])\n",
    "plot_decision_function(kernel_model, range_features_moons, ax=ax)\n",
    "_ = plt.title(\"Decision boundary with a model using an RBF kernel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7694042",
   "metadata": {},
   "source": [
    "We see that the decision boundary is not anymore a straight line. Indeed,\n",
    "an area is defined around the red samples and we could imagine that this\n",
    "classifier should be able to generalize on unseen data.\n",
    "\n",
    "Let's check the decision function on the second dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a430b2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_model.fit(data_gauss, target_gauss)\n",
    "ax = sns.scatterplot(data=gauss, x=feature_names[0], y=feature_names[1],\n",
    "                     hue=target_gauss, palette=[\"tab:red\", \"tab:blue\"])\n",
    "plot_decision_function(kernel_model, range_features_gauss, ax=ax)\n",
    "_ = plt.title(\"Decision boundary with a model using an RBF kernel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fa53a8",
   "metadata": {},
   "source": [
    "We observe something similar than in the previous case. The decision function\n",
    "is more flexible and does not underfit anymore.\n",
    "\n",
    "Thus, kernel trick or feature expansion are the tricks to make a linear\n",
    "classifier more expressive, exactly as we saw in regression.\n",
    "\n",
    "Keep in mind that adding flexibility to a model can also risk increasing\n",
    "overfitting by making the decision function to be sensitive to individual\n",
    "(possibly noisy) data points of the training set. Here we can observe that\n",
    "the decision functions remain smooth enough to preserve good generalization.\n",
    "If you are curious, you can try to repeat the above experiment with\n",
    "`gamma=100` and look at the decision functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33af7d1b-d1d7-4af9-9789-faf64969106c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
