{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82a15d6a",
   "metadata": {},
   "source": [
    "# Gradient Boosting Decision tree - exercise\n",
    "\n",
    "The aim of this exercise is to:\n",
    "\n",
    "* verify if a GBDT tends to overfit if the number of estimators is not\n",
    "  appropriate as previously seen for AdaBoost;\n",
    "* use the early-stopping strategy to avoid adding unnecessary trees, to\n",
    "  get the best generalization performances.\n",
    "\n",
    "We will use the California housing dataset to conduct our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317acc10-3e38-4c23-934c-375fc07a26ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c48f383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data, target = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "target *= 100  # rescale the target in k$\n",
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    data, target, random_state=0, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ee76ab8-a251-46c1-a95a-879a10cbb9dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Split arrays or matrices into random train and test subsets\n",
       "\n",
       "Quick utility that wraps input validation and\n",
       "``next(ShuffleSplit().split(X, y))`` and application to input data\n",
       "into a single call for splitting (and optionally subsampling) data in a\n",
       "oneliner.\n",
       "\n",
       "Read more in the :ref:`User Guide <cross_validation>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "*arrays : sequence of indexables with same length / shape[0]\n",
       "    Allowed inputs are lists, numpy arrays, scipy-sparse\n",
       "    matrices or pandas dataframes.\n",
       "\n",
       "test_size : float or int, default=None\n",
       "    If float, should be between 0.0 and 1.0 and represent the proportion\n",
       "    of the dataset to include in the test split. If int, represents the\n",
       "    absolute number of test samples. If None, the value is set to the\n",
       "    complement of the train size. If ``train_size`` is also None, it will\n",
       "    be set to 0.25.\n",
       "\n",
       "train_size : float or int, default=None\n",
       "    If float, should be between 0.0 and 1.0 and represent the\n",
       "    proportion of the dataset to include in the train split. If\n",
       "    int, represents the absolute number of train samples. If None,\n",
       "    the value is automatically set to the complement of the test size.\n",
       "\n",
       "random_state : int, RandomState instance or None, default=None\n",
       "    Controls the shuffling applied to the data before applying the split.\n",
       "    Pass an int for reproducible output across multiple function calls.\n",
       "    See :term:`Glossary <random_state>`.\n",
       "\n",
       "\n",
       "shuffle : bool, default=True\n",
       "    Whether or not to shuffle the data before splitting. If shuffle=False\n",
       "    then stratify must be None.\n",
       "\n",
       "stratify : array-like, default=None\n",
       "    If not None, data is split in a stratified fashion, using this as\n",
       "    the class labels.\n",
       "    Read more in the :ref:`User Guide <stratification>`.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "splitting : list, length=2 * len(arrays)\n",
       "    List containing train-test split of inputs.\n",
       "\n",
       "    .. versionadded:: 0.16\n",
       "        If the input is sparse, the output will be a\n",
       "        ``scipy.sparse.csr_matrix``. Else, output type is the same as the\n",
       "        input type.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> import numpy as np\n",
       ">>> from sklearn.model_selection import train_test_split\n",
       ">>> X, y = np.arange(10).reshape((5, 2)), range(5)\n",
       ">>> X\n",
       "array([[0, 1],\n",
       "       [2, 3],\n",
       "       [4, 5],\n",
       "       [6, 7],\n",
       "       [8, 9]])\n",
       ">>> list(y)\n",
       "[0, 1, 2, 3, 4]\n",
       "\n",
       ">>> X_train, X_test, y_train, y_test = train_test_split(\n",
       "...     X, y, test_size=0.33, random_state=42)\n",
       "...\n",
       ">>> X_train\n",
       "array([[4, 5],\n",
       "       [0, 1],\n",
       "       [6, 7]])\n",
       ">>> y_train\n",
       "[2, 0, 3]\n",
       ">>> X_test\n",
       "array([[2, 3],\n",
       "       [8, 9]])\n",
       ">>> y_test\n",
       "[1, 4]\n",
       "\n",
       ">>> train_test_split(y, shuffle=False)\n",
       "[[0, 1, 2], [3, 4]]\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/local/lib/python3.9/site-packages/sklearn/model_selection/_split.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_test_split?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c081459",
   "metadata": {},
   "source": [
    "Similarly to the previous exercise, create a gradient boosting decision tree\n",
    "and create a validation curve to assess the impact of the number of trees\n",
    "on the generalization performance of the model. Use the mean absolute error\n",
    "to assess the generalization performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ea9b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import validation_curve\n",
    "# Write your code here.\n",
    "\n",
    "param_range = np.unique(np.logspace(0, 1.8, num=30)\n",
    "# Write your code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c846d2c3-b1d1-4f75-a497-2b3e5559aa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.errorbar(\n",
    "    param_range,\n",
    "    train_errors.mean(axis=1),\n",
    "    yerr=train_errors.std(axis=1),\n",
    "    label=\"Training score\",\n",
    ")\n",
    "plt.errorbar(\n",
    "    param_range,\n",
    "    test_errors.mean(axis=1),\n",
    "    yerr=test_errors.std(axis=1),\n",
    "    label=\"Cross-validation score\",\n",
    ")\n",
    "\n",
    "plt.legend()\n",
    "plt.ylabel(\"Mean absolute error in k$\\n(smaller is better)\")\n",
    "plt.xlabel(\"# estimators\")\n",
    "_ = plt.title(\"Validation curve for GBDT regressor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e25f4ce",
   "metadata": {},
   "source": [
    "Unlike AdaBoost, the gradient boosting model will always improve when\n",
    "increasing the number of trees in the ensemble. However, it will reach a\n",
    "plateau where adding new trees will just make fitting and scoring slower.\n",
    "\n",
    "To avoid adding new unnecessary tree, gradient boosting offers an\n",
    "early-stopping option. Internally, the algorithm will use an out-of-sample\n",
    "set to compute the generalization performance of the model at each addition of a\n",
    "tree. Thus, if the generalization performance is not improving for several\n",
    "iterations, it will stop adding trees.\n",
    "\n",
    "Now, create a gradient-boosting model with `n_estimators=1000`. This number\n",
    "of trees will be too large. Change the parameter `n_iter_no_change` such\n",
    "that the gradient boosting fitting will stop after adding 5 trees that do not\n",
    "improve the overall generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94cacb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
