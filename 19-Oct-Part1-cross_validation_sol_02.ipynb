{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89376a80",
   "metadata": {},
   "source": [
    "# Solution for Baseline performance exercise\n",
    "\n",
    "This notebook aims at building baseline classifiers, which we'll use to\n",
    "compare our predictive model. Besides, we will check the differences with\n",
    "the baselines that we saw in regression.\n",
    "\n",
    "We will use the adult census dataset, using only the numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b5f223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "adult_census = pd.read_csv(\"../datasets/adult-census-numeric-all.csv\")\n",
    "data, target = adult_census.drop(columns=\"class\"), adult_census[\"class\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438a2795",
   "metadata": {},
   "source": [
    "First, define a `ShuffleSplit` cross-validation strategy taking half of the\n",
    "sample as a testing at each round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54462391",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa3b571",
   "metadata": {},
   "source": [
    "Next, create a machine learning pipeline composed of a transformer to\n",
    "standardize the data followed by a logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1443eaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = make_pipeline(StandardScaler(), LogisticRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33626a19",
   "metadata": {},
   "source": [
    "Get the test score by using the model, the data, and the cross-validation\n",
    "strategy that you defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03564be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "result_classifier = cross_validate(classifier, data, target, cv=cv, n_jobs=2)\n",
    "\n",
    "test_score_classifier = pd.Series(\n",
    "    result_classifier[\"test_score\"], name=\"Classifier score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9264f91",
   "metadata": {},
   "source": [
    "Using the `sklearn.model_selection.permutation_test_score` function,\n",
    "check the chance level of the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a37510",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import permutation_test_score\n",
    "\n",
    "score, permutation_score, pvalue = permutation_test_score(\n",
    "    classifier, data, target, cv=cv, n_jobs=2, n_permutations=10)\n",
    "test_score_permutation = pd.Series(permutation_score, name=\"Permuted score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9358e4",
   "metadata": {},
   "source": [
    "Finally, compute the test score of a dummy classifier which would predict\n",
    "the most frequent class from the training set. You can look at the\n",
    "`sklearn.dummy.DummyClassifier` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6128ae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy = DummyClassifier(strategy=\"most_frequent\")\n",
    "result_dummy = cross_validate(dummy, data, target, cv=cv, n_jobs=2)\n",
    "test_score_dummy = pd.Series(result_dummy[\"test_score\"], name=\"Dummy score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0145b1",
   "metadata": {},
   "source": [
    "Now that we collected the results from the baselines and the model, plot\n",
    "the distributions of the different test scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9c0d01",
   "metadata": {},
   "source": [
    "We concatenate the different test score in the same pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c41711",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_scores = pd.concat(\n",
    "    [test_score_classifier, test_score_permutation, test_score_dummy],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9c01e8",
   "metadata": {},
   "source": [
    "Next, plot the distributions of the test scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bf3258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "final_test_scores.plot.hist(bins=50, density=True, edgecolor=\"black\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 0.8), loc=\"upper left\")\n",
    "plt.xlabel(\"Accuracy (%)\")\n",
    "_ = plt.title(\"Distribution of the test scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621e78f1",
   "metadata": {},
   "source": [
    "We observe that the dummy classifier with the strategy `most_frequent` is\n",
    "equivalent to the permutation score. We can also conclude that our model\n",
    "is better than the other baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e2490c",
   "metadata": {},
   "source": [
    "Change the strategy of the dummy classifier to `stratified`, compute the\n",
    "results and plot the distribution together with the other results. Explain\n",
    "why the results get worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250f69d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyClassifier(strategy=\"stratified\")\n",
    "result_dummy_stratify = cross_validate(dummy, data, target, cv=cv, n_jobs=2)\n",
    "test_score_dummy_stratify = pd.Series(\n",
    "    result_dummy_stratify[\"test_score\"], name=\"Dummy 'stratify' score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3249b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_scores = pd.concat(\n",
    "    [\n",
    "        test_score_classifier, test_score_permutation,\n",
    "        test_score_dummy, test_score_dummy_stratify,\n",
    "    ],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19540887",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_scores.plot.hist(bins=50, density=True, edgecolor=\"black\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 0.8), loc=\"upper left\")\n",
    "plt.xlabel(\"Accuracy (%)\")\n",
    "_ = plt.title(\"Distribution of the test scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67b237c",
   "metadata": {},
   "source": [
    "We see that using `strategy=\"stratified\"`, the results are much worse than\n",
    "with the `most_frequent` strategy. Since the classes are imbalanced,\n",
    "predicting the most frequent involves that we will be right for the\n",
    "proportion of this class (~75% of the samples). However, by using the\n",
    "`stratified` strategy, wrong predictions will be made even for the most\n",
    "frequent class, hence we obtain a lower accuracy."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
